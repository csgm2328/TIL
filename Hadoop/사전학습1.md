# 사전학습 1
* 기존의 컴퓨팅 기술로는 처리할 수 없는 대규모 데이터 세트 모음
  * ex) 구조화된 데이터(관계형), 반구조화(XML), 비정형(Word,pdf,텍스트)
* 이러한 빅데이터를 활용해 광고 효과, 환자 병력과 같은 더 나은 서비스를 위한 의사결정에 사용
* 이를 위해서 정형/비정형 데이트를 실시간으로 관리하고 처리할 인프라 필요 + 보안
* NoSQL은 대규모 계산을 저렴하고 효율적으로 실행하도록 설계됨(MongoDB)
  * MapReduce가 SQL 기능을 보완하고 scale out이 가능하도록 시스템을 제공함.


* 빅데이터 솔루션
  * 수용할 수 없는 막대한 양의 데이터를 처리할 때 병목현상이 발생하는데 이를 해결할 수 있는 솔루션이 MapReduce 알고리즘.
  * MapReduce 알고리즘
    * 작업을 작은 부분으로 나눠 많은 컴퓨터에 할당하고 그 결과 데이터 셋트를 수집한다.
    * 데이터들이 병렬로 처리되어 병목현상 해결
  * Hadoop
    * MapReduce 알고리즘을 사용하여 앱을 실행한다.
    * 고로 여러 MR 알고리즘을 쓸수도 있는 것.
    * 대규모 데이터 세트를 분산처리하는 프레임워크로 말한다.
    * 하둡의 클러스터는 각각 로컬 스토리지와 계산하는 환경을 가진다.
    * 아키텍처
      * 처리/계산 Layer(MR)
      * 스토리지(HDFS)
      <img src="..\assets\hadoop_architecture.JPG">
    * MapReduce
      * 내결함성방식의 병렬 분산 처리 프로그래밍 모델
    * 하둡 분산 파일 시스템(HDFS)
      * 구글 파일 시스템을 기반으로 기존의 분산 파일 시스템과의 차이점은 내결함성이 뛰어나고 저렴한 하드웨어에 배포가능함.
    * Hadoop Common
      * 하둡 모듈에 필요한 JAVA 라이브러리 및 유틸
    * Hadoop YARN
      * 작업 스케줄링 및 클러스터 리소스 관리를 위한 프레임워크
  * 하둡의 작동
    * 컴퓨터 클러스터에서 코드 실행
      * 데이터는 초기에 directory / file로 나뉨
      * 디렉토리는 128MB(혹은 64MB)의 균일한 크기의 블록으로 나뉨
      * 그 후, 추가처리를 위해 클러스터 노드에 배포됨
      * 로컬 파일시스템 위에 있는 HDFS가 처리를 감독
      * 블록 복제
      * 성공 확인
      * 맵과 리듀스 단계 사이 에서 정렬 수행
      * 정렬된 데이터를 특정 컴퓨터로 전송
      * 디버깅 로그
  * 하둡의 장점
    * 데이터와 작업을 머신 전체에 자동 배포
    * 하둡은 FTHA(내결함성 및 고가용성)을 제공하기 위해 하드웨어에 의존하지 않으며, 애플리케이션 계층에서 오류감지와 처리
    * 중단없이 클러스터에서 서버 추가 삭제가능
    * Java 기반, 오픈소스라 모든 플랫폼에서 호환가능
* HDFS
  * 마스터 - 슬레이브 아키텍처
  * 네임 노드
    * 파일시스템 네임스페이스 관리, 파일에 대한 클라이언트 액세스 규제
    * 파일 열기,닫기
  * 데이터 노드
    * 시스템의 데이터 저장소 관리
    * 클라이언트 요청에 따라 읽기쓰기 작업 수행
    * 네임 노드 지시로 생성, 삭제,복제 수행
  * 블록
    * 파일이 저장되는 단위(파일 세그먼트)
    * 읽고 쓰느 최소 데이터 양
    * 기본 64MB
  * 목표
    * 구성된 하드웨어의 빠른 오류 감지 및 복구
    * 수백개의 노드로 빅데이터 세트를 가진 앱 관리
    * 효율적 데이터 처리

* MapReduce
  * Map(변환) + Reduce(결합) 작업
  * Map
    * 데이터 세트를 다른 데이터세트로 변환
    * 요소들은 Tuple(key-value)
  * Reduce
    * Map의 출력을 입력으로 받아 데이터 튜플을 더 작은 튜플 세트로 결합하는 작업
  * 미리 MapReduce로 앱을 작성했다면 앱을 확장하는것은 일도 아니다.
  * 알고리즘
    * Map 단계
      * HDFS에 저장된 입력 데이터를 처리하여 여러 개의 작은 데이터 청크로 생성
      * 같은 키를 가진 쌍은 같은 머신으로 보내짐
    * Reduce 단계
      * Shuffle 단계와의 조합 매퍼에서 가져온 데이터로 새로운 출력 셋트를 생성
      * Shuffle 단계에서 KEY로 정렬 후 같이 키가진 값들을 리스트로 모아서 다시 머신으로 보냄
    * 하둡은 이 단계를 적절한 서버가 처리하도록 하고 서버에서 처리된 결과는 다시 하둡 서버로 돌아옴
    <img src="..\assets\hadoop_algorithm.JPG">
  * 입출력
    <img src="..\assets\hadoop_io.JPG">
  * 정리
    * 고로 MapReduce가 소프트웨어 수행을 분산하고 HDFS가 데이터를 분산한다
  * 함수
    * Map()
      * setup()
        * Main()으로부터 모든 Map()함수들에게 전달해야 할 파라미터 정보들 브로드캐스트
        * Map() 함수들이 공유하는 자료구조 초기화
        * clenup()
          * 공유 자료구조 결과 출력
          * reduce() 안하고 끝낼 수도 있음
      * 텍스트 파일 라인 단위로 호출됨
      * KEY는 해당 라인의 첫번째 문자까지의 offset
      * value는 라인 전체
      <img src="..\assets\hadoop_map().JPG">
      * 그 후 KEY에다 해시함수를 적용해 보낼 머신 정하기
    * Shuffling phase
      * 각 머신에서 정렬하고 value list로 만들어줌
     * Combine()
      * Map()함수의 결과 크기를 줄여줌
      * 셔플링 비용과 네트웍 트래픽을 줄여줌
      * partially aggregated result 리턴함
      * <img src="..\assets\hadoop_reduce().JPG">

* 환경설정
  * WSL 2 설치
  * repo 오류 나므로 한번 재부팅
    * powershell에서 wsl --shutdown
  * 재부팅 하면 ssh 포트 오류도 해결됨
  
* 실행
  * linux 디렉토리에 있는 데이터를
  * HDFS로 옮겨야함
    * HDFS에서 입출력 디렉토리 만들어주고
      ```java
      // 메인함수에서 Mapper나 reducer에서 사용할 변수를 브로드캐스트한다.
      Main_setup(){
        contex.Configration(); //
        name = config.get("name", "chu"); // 메인에서 값 가져오는데 없다면 default 값 "chu"
        point = config.getInt("one", 1); 
      }
      ````
  * map --> 2개의 reducer를 거쳐 출력 파일을 만들면
    * out/part-r-00000 ~ 1
    * 입력을 두개의 reducer로 쪼개서 작업했기 때문에 각각의 결과를 가진다.